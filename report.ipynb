{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group Name**: support nectar machines\n",
    "\n",
    "**Team members**: David Hofer (Cyber Security), Frederieke Lohmann (Data Science), Arvid Ban (Computer Science), Yi-Yi Ly (Neuroinformatics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our solution for the UBS Challenge at Datathon 2024, we pursue a diverse approach of modeling brand interaction data on social media. Our aim is to provide investors with a broad information base and sophisticated insights based on different methods using the data provided by the UBS Evidence Lab which tracks the popularity of brands on Instagram. Examples of the data collected are for example number of followers, pictures, videos, comments and likes.\n",
    "\n",
    "Our methods focus both on identifying viral brands and outliers based on various features and embeddings, and on predictive model of performance metrics useful for investors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "The dataset consists of 704'313 rows from  **706 brands** that were recorded from 2015-01-03 to 2023-09-16. The brands are grouped into 20 main competitive sets that vary in size from 1 brands to 164 brands. The dataset contains information such as legal entity and stock exchange name that could be used to augment the dataset with financial information, as well as the actual interaction data in the form of likes, followers, comments, pictures and videos.\n",
    "\n",
    "![Correlation matrix](corrmatrix.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figure above displays the correlations between the main numerical features. It shows for example that pictures are highly correlated with likes and comments, more than e.g. videos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and preprocessing\n",
    "\n",
    "In our preprocessing, we removed a large number of features that we deemed generally uninformative, as well as the two constant features `period` and `calculation_type`. Our primary focus was on modeling the social media user interaction, thus columns like `legal_entity_name`, `ultimate_parent_legal_entity_name`, `primary_exchange_name` did not provide any additional insights to us. Furthermore, there were lots of duplicate data rows with the only difference being the `compset` value, leading us to reject the feature as it did not seem to be informative. And finally, as our investigation of the `compset_group` feature found that most groups only contain a single or few brands and is thus not suitable for comparative analysis between in-group brands, we also left it out. We \n",
    "We experimented both with and without the `domicile_country_name` (which we previously cleaned by replacing certain noisy strings), but as it did not lead to a significant improvement of our methods, we opted towards simplicity and left it out too.\n",
    "We also assumed that many of these aforementioned features are anyway strongly correlated among each other and with the brand, leading to dimishing returns in including them.\n",
    "\n",
    "In the next step, we completely removed brands with too much missing data in one of the five numerical features `brand`, `followers`, `pictures`, `videos`, `comments`, and `likes`, based on a 70% threshold.\n",
    "\n",
    "We also standardized all numerical features by subtracting the mean and dividing by the standard deviation. We then imputed the remaining values in the time series by using the forward-fill method, with the intention of not leaking future data. In the few cases where an initial datapoint was missing, we imputed it using the first value occuring in the sequence.\n",
    "\n",
    "Lastly, we also created a train-test split for evaluation. For this, we grouped all the time series by brand, and for each brand cut off the last 20% of the datapoints as a test set.\n",
    "\n",
    "\n",
    "Our philosophy was to largely focus on the raw features and let our models learn directly from them. However, we experimented with various engineered advanced features, such as rolling averages, exponential moving averages, growth rates, rolling minimum, maximum, and standard deviation, as well as time lag features (older features from previous timesteps shifted forward). In the end, we decided to augment our features with the rolling min, max, and std as well as time lag features.\n",
    "\n",
    "We also came up with custom user brand engagement metrics, namely the `engagement_rate_per_post`, which is calculated as $erpp = \\frac{likes + comments}{followers * (pictures + videos)}$. This metric counts the number of interactions on a weekly basis with the brand, normalized by the frequency of posts as well as the size of the user base.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pursued two different approaches for modeling the trends in the social media interaction dataset. The first one\n",
    "\n",
    "### LSTM prediction task and embedding\n",
    "The second approach focused on using a RNN, specifically a LSTM, to model the data. RNNs are suitable for this kind of time series data, as they are able to aggregate and remember an internal state over multiple time steps and can use this state for further downstream tasks. We used this in two ways. First, we trained our LSTM in a supervised way to predict our engagement metric `engagement_rate_per_post` 4 weeks in the future, after seeing 10 weeks of features (10 timesteps). Given a contiguous time series for a specific brand, this time series is split into individual samples with distinct start and end sequence dates, always predicting the metric 4 weeks ahead of the last sequence datapoint. The idea here is that if the model is able to accurately solve this task, it can be used to predict user engagement with a brand in one month worth of time, based on the recent history and rather simple features based on it, thus modelling and predicting the future engagement with the brand. By analyzing specific brands, one can identify opportunities or threats by detecting changes in engagement early. \n",
    "\n",
    "Our predictive model consists of a 2-layer LSTM with hidden size 64. The hidden state of the last input feature is fed into a two-layer MLP, with hidden size 64, ReLU activation and a single output node. The model is then trained on input sequences of length 10 with label `engagement_rate_per_post` derived from the features in 4 weeks time. Hyperparameters and training setup used are batch size 64, learning rate 0.001, Adam optimizer, and 20 epochs training with MSE loss.\n",
    "We evaluated the performance of our model on a test set and compared to the performance of our baseline, a Linear Regression model fitted to all the last datapoints in the training sequences. On the test set, the LSTM had a MSE of 0.78, compared to the MSE of the Linear Regression model of 1.21.\n",
    "\n",
    "![LSTM architecture](architecture.jpeg)\n",
    "\n",
    "The second application for our LSTM is due to the fact that we can understand the internal state of the model as an embedding of the recent brand history in terms of user interaction and posting activity. Using this embedding, obtained by feeding in features from $n$ consecutive datapoints and extracting the hidden state afterwards, can be used in a multitude of ways. \n",
    "### TODO outlier detection hidden state\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One big avenue for further investigation is the analysis of the embeddings computed by our LSTM. This approach requires more detailed evaluation of the information encoded in the embeddings, as well as methods for clustering the embeddings and detecting outliers and trends based on the embeddings. \n",
    "\n",
    "This would also benefit from a more sophisticated performance metric for brands on social media, potentially augmented by external data. Such an improved performance metric would force the LSTM model to learn and extract representative and insigthful features from the input data and embed them in a latent space during its prediction task in order to perform well.\n",
    "\n",
    "Insightful performance metrics could also help in evaluating brands identified by different methods such as outlier and trend detection through clustering or the predictive modelling approach.\n",
    "\n",
    "The prediction task would also benefit from investments in hyperparameter tuning and experimenting with the model architecture, as we only had limited time to investigate this. A competitive alternative in terms of architecture to LSTMs are GRUs, allowing for comparison between the two RNN types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlook\n",
    "\n",
    "We think that our approach presents a promising direction for modeling and understanding brand trends on social media. It allows investors to predict user engagement ahead of time, one month in advance, enables them to identify brands going viral based on identifying spikes, and opens the door for interesting further research in the direction of brand history embeddings used for anomaly detection. We think that in this dynamic area, a diversified modeling approach is crucial to guarantee the edge in identifying winning brands, and tried to show such an approach in our project.\n",
    "\n",
    "# TODO REcommender"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
