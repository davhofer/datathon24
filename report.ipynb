{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Progress Report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Group Name**: support nectar machines\n",
    "\n",
    "**Team members**: David Hofer (Cyber Security), Frederieke Lohmann (Data Science), Arvid Ban (Computer Science), Yi-Yi Ly (Neuroinformatics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of our hacking project is to create a **recommender system for investors**. This recommender system uses the data provided by the UBS Evidence Lab which tracks the popularity of brands on Instagram. Examples of the data collected are for example number of followers, pictures, videos, comments and likes.\n",
    "\n",
    "Our recommender system personalizes the information learnt from the data by the following criteria of the investor:\n",
    "1. Popularity prospects based on **current** versus **future predicted** metrics\n",
    "1. Type of investment desired, i.e. **risk-taking** versus **conservative** investment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory Data Analysis\n",
    "\n",
    "The dataset consists of 704'313 rows from  **706 brands** that were recorded from 2015-01-03 to 2023-09-16. The brands are grouped into 20 main competitive sets that vary in size from 1 brands to 164 brands."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Figure caption](../yiyi/figs/main_comp_size.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data cleaning and preprocessing\n",
    "\n",
    "> Provide a detailed account of the initial steps taken\n",
    "to prepare the data for analysis. This should include a description of how data quality\n",
    "issues, such as missing values or outliers, were addressed.\n",
    "\n",
    "> Assumptions: Clearly articulate any assumptions that were made during the data\n",
    "preparation phase.\n",
    "\n",
    "In our preprocessing, we removed a large number of features that we deemed generally uninformative, as well as the two constant features `period` and `calculation_type`. Our primary focus was on modeling the social media user interaction, thus columns like `legal_entity_name`, `ultimate_parent_legal_entity_name`, `primary_exchange_name` did not provide any additional insights to us. Furthermore, there were lots of duplicate data rows with the only difference being the `compset` value, leading us to reject the feature as it did not seem to be informative. And finally, as our investigation of the `compset_group` feature found that most groups only contain a single or few brands and is thus not suitable for comparative analysis between in-group brands, we also left it out. We \n",
    "We experimented both with and without the `domicile_country_name` (which we previously cleaned by replacing certain noisy strings), but as it did not lead to a significant improvement of our methods, we opted towards simplicity and left it out too.\n",
    "We also assumed that many of these aforementioned features are anyway strongly correlated among each other and with the brand, leading to dimishing returns in including them.\n",
    "\n",
    "In the next step, we completely removed brands with too much missing data in one of the five numerical features `brand`, `followers`, `pictures`, `videos`, `comments`, and `likes`, based on a 70% threshold.\n",
    "\n",
    "We also standardized all numerical features by subtracting the mean and dividing by the standard deviation. We then imputed the remaining values in the time series by using the forward-fill method, with the intention of not leaking future data. In the few cases where an initial datapoint was missing, we imputed it using the first value occuring in the sequence.\n",
    "\n",
    "Lastly, we also created a train-test split for evaluation. For this, we grouped all the time series by brand, and for each brand cut off the last 20% of the datapoints as a test set.\n",
    "\n",
    "\n",
    "> Feature engineering and data augmentation: Describe any techniques employed to\n",
    "enhance the dataset, whether through the creation of new features or augmentation\n",
    "of the existing data.\n",
    "\n",
    "Our philosophy was to largely focus on the raw features and let our models learn directly from them. However, we experimented with various engineered advanced features, such as rolling averages, exponential moving averages, growth rates, rolling minimum, maximum, and standard deviation, as well as time lag features (older features from previous timesteps shifted forward). In the end, we decided to augment our features with the rolling min, max, and std as well as time lag features.\n",
    "\n",
    "We also came up with custom user brand engagement metrics, namely the `engagement_rate_per_post`, which is calculated as $erpp = \\frac{likes + comments}{followers * (pictures + videos)}$. This metric counts the number of interactions on a weekly basis with the brand, normalized by the frequency of posts as well as the size of the user base.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Modeling Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We pursued two different approaches for modeling the trends in the social media interaction dataset. The first one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model enhancement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
