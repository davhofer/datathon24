{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observations\n",
    "\n",
    "- period and calculation_type features all the same value, can drop\n",
    "- split time series into batches of shorter sequences\n",
    "- how to impute intermediate missing values? -> take last? interpolate?\n",
    "- add year, month, absolute int as features for date\n",
    "\n",
    "- one hot encoding of categorical features: too many, not important enough... drop? keep as single numerical feature?\n",
    "\n",
    "- duplicate datapoints, same brand, same date, same values - two different compsets...\n",
    "\n",
    "# TODO:\n",
    "- align the data by brand\n",
    "- are there weeks missing in between?\n",
    "- normalize values\n",
    "\n",
    "\n",
    "# Discussion\n",
    "- should we predict by brand or by legal entity?\n",
    "\n",
    "\n",
    "- standardization in the end after adding additional metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = \"/home/david/code/datathon24-personal/data/skylab_instagram_datathon_dataset.csv\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(DATA_PATH, sep=\";\")\n",
    "\n",
    "\n",
    "def video_to_picture_ratio(df, videos_col, pictures_col):\n",
    "    df['video_picture_ratio'] = df[videos_col] / df[pictures_col]\n",
    "    return df\n",
    "\n",
    "\n",
    "def calculate_rolling_average_per_brand(data, brand_column, window_size=7):\n",
    "    \"\"\"\n",
    "    Calculate the rolling average of 'engagement' for each brand in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The DataFrame containing the data.\n",
    "        brand_column (str): The column name which identifies the brand.\n",
    "        window_size (int): The number of observations used for calculating the rolling average.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with a new column for the rolling average of 'engagement' calculated per brand.\n",
    "    \"\"\"\n",
    "    # Group by brand and calculate rolling average within each group\n",
    "    data['rolling_avg_engagement'] = data.groupby(brand_column)['engagement'].transform(lambda x: x.rolling(window=window_size, min_periods=1).mean())\n",
    "    return data\n",
    "\n",
    "\n",
    "def calculate_exponential_moving_average_per_brand(data, brand_column, span=7):\n",
    "    \"\"\"\n",
    "    Calculate the exponential moving average of 'engagement' for each brand in a DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The DataFrame containing the data.\n",
    "        brand_column (str): The column name which identifies the brand.\n",
    "        span (int): The decay in terms of the span of the exponential window.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with a new column for the exponential moving average of 'engagement' calculated per brand.\n",
    "    \"\"\"\n",
    "    # Group by brand and calculate EMA within each group\n",
    "    data['ewma_engagement'] = data.groupby(brand_column)['engagement'].transform(lambda x: x.ewm(span=span, adjust=False).mean())\n",
    "    return data\n",
    "\n",
    "\n",
    "def calculate_brand_wise_growth_rates(data, column_names, brand_column):\n",
    "    \"\"\"\n",
    "    Calculate the growth rate for specified columns in a DataFrame, grouped by brand.\n",
    "    Replace NaN values with 0 where growth rate cannot be calculated.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The DataFrame containing the data.\n",
    "        column_names (list of str): A list of column names to calculate the growth rate.\n",
    "        brand_column (str): The column name which identifies the brand.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with new columns for each specified column's growth rate, calculated for each brand and NaN replaced by 0.\n",
    "    \"\"\"\n",
    "    for column in column_names:\n",
    "        # Calculate growth rate within each brand group\n",
    "        data[f'growth_rate_{column}'] = data.groupby(brand_column)[column].pct_change() * 100\n",
    "        # Replace NaN values with 0\n",
    "        data[f'growth_rate_{column}'].fillna(0, inplace=True)\n",
    "    return data\n",
    "\n",
    "def calculate_brand_rolling_statistics(data, column_name, brand_column, window_size=7):\n",
    "    \"\"\"\n",
    "    Calculate rolling statistics for a specified column in a DataFrame, grouped by brand,\n",
    "    and replace NaN values with 0.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The DataFrame containing the data.\n",
    "        column_name (str): The column name to calculate rolling statistics.\n",
    "        brand_column (str): The column name which identifies the brand.\n",
    "        window_size (int): The number of observations used for calculating the rolling statistic.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with new columns for each rolling statistic of the specified column, calculated for each brand.\n",
    "    \"\"\"\n",
    "    grouped = data.groupby(brand_column)[column_name]\n",
    "    data[f'{column_name}_rolling_min'] = grouped.transform(lambda x: x.rolling(window=window_size, min_periods=1).min()).fillna(0)\n",
    "    data[f'{column_name}_rolling_max'] = grouped.transform(lambda x: x.rolling(window=window_size, min_periods=1).max()).fillna(0)\n",
    "    data[f'{column_name}_rolling_std'] = grouped.transform(lambda x: x.rolling(window=window_size, min_periods=1).std()).fillna(0)\n",
    "\n",
    "    return data\n",
    "\n",
    "def create_brand_lag_features(data, column_name, brand_column, lag_periods):\n",
    "    \"\"\"\n",
    "    Create lag features for a specified column in a DataFrame, grouped by brand,\n",
    "    and replace NaN values with 0.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The DataFrame containing the data.\n",
    "        column_name (str): The column name to create lag features for.\n",
    "        brand_column (str): The column name which identifies the brand.\n",
    "        lag_periods (int): The number of lag periods.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with new columns for each lag feature of the specified column, calculated for each brand.\n",
    "    \"\"\"\n",
    "    for i in range(1, lag_periods + 1):\n",
    "        data[f'{column_name}_lag_{i}'] = data.groupby(brand_column)[column_name].shift(i).fillna(0)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def preprocess_data(df, missing_values_cutoff=0.7, test_fraction=0.2):\n",
    "\n",
    "    df = df.drop(columns=[\"period\", \"calculation_type\", \"compset\", \"compset_group\", \"legal_entity_name\", \"ultimate_parent_legal_entity_name\", \"primary_exchange_name\"])\n",
    "    df[\"period_end_date\"] = pd.to_datetime(df[\"period_end_date\"])\n",
    "\n",
    "    df = df.rename(columns={'business_entity_doing_business_as_name': 'brand', 'period_end_date': 'date'})\n",
    "\n",
    "    df = df[df['brand'] != \"All Brands\"]\n",
    "    df = df[df['brand'] != \"Boca\"]\n",
    "    df = df[df['brand'] != \"Bulgari Beauty\"]\n",
    "\n",
    "    df = df.groupby(['brand', 'date']).first().reset_index()\n",
    "\n",
    "    def correct_country_name(name):\n",
    "        country_map = {\n",
    "            \"Hong Kong\": \"China\",\n",
    "            \"China;Hong Kong\": \"China\",\n",
    "            \";France\": \"France\",\n",
    "            \";\": None,\n",
    "            \"Belgium;\": \"Belgium\"\n",
    "        }\n",
    "        if name in country_map.keys():\n",
    "            return country_map[name]\n",
    "        return name\n",
    "\n",
    "    df['domicile_country_name'] = df['domicile_country_name'].apply(correct_country_name)\n",
    "\n",
    "\n",
    "    categorical_features = [\"domicile_country_name\"]\n",
    "    for feature in categorical_features:\n",
    "        df = pd.get_dummies(df, columns=[feature], prefix=feature, dummy_na=True, dtype=int)\n",
    "\n",
    "    # TODO: remove bad data\n",
    "    na_frac = df[['brand', 'followers', 'pictures',\n",
    "        'videos', 'comments', 'likes']].groupby('brand').apply(lambda x: x.iloc[:,1:].isna().sum()/len(x))\n",
    "\n",
    "    bad_brands = list(na_frac[na_frac.max(axis=1) > missing_values_cutoff].index)\n",
    "\n",
    "    df = df[~df['brand'].isin(bad_brands)]\n",
    "\n",
    "\n",
    "    # TODO: add additional features\n",
    "    df['engagement'] = df['comments'] + df['likes']\n",
    "    df['engagement_rate'] = df['engagement']/df['followers']\n",
    "    df['engagement_rate_per_post'] =  df['engagement_rate']/(df['videos'] + df['pictures'])\n",
    "\n",
    "\n",
    "    # df = video_to_picture_ratio(df, \"videos\", \"pictures\")\n",
    "    df = calculate_rolling_average_per_brand(df, \"brand\", window_size=4)\n",
    "    df = calculate_exponential_moving_average_per_brand(df, \"brand\", span=4)\n",
    "    # df = calculate_brand_wise_growth_rates(df, [\"comments\", \"likes\", \"followers\", \"pictures\", \"videos\"], \"brand\")\n",
    "    for c in [\"comments\", \"likes\", \"followers\", \"pictures\", \"videos\"]:\n",
    "        df = calculate_brand_rolling_statistics(df, c, \"brand\", window_size=4)\n",
    "        df = create_brand_lag_features(df, c, \"brand\", 4)\n",
    "\n",
    "    \n",
    "    # Step 1: Sort the dataframe by time\n",
    "    df_sorted = df.sort_values(by='date')\n",
    "\n",
    "    # Step 2: Group the dataframe by 'Brand'\n",
    "    grouped = df_sorted.groupby('brand')\n",
    "\n",
    "    # Step 3: Define an empty dataframe for train and test sets\n",
    "    train_df = pd.DataFrame(columns=df.columns)  # Columns same as original dataframe\n",
    "    test_df = pd.DataFrame(columns=df.columns)   # Columns same as original dataframe\n",
    "\n",
    "    # Step 4: Iterate over each group and split into train and test sets\n",
    "    for _, group in grouped:\n",
    "        n_rows = len(group)\n",
    "        n_test = int(test_fraction * n_rows)  # 20% of rows for test set\n",
    "\n",
    "        # Add last 20% of rows to test set\n",
    "        test_df = pd.concat([test_df, group.iloc[-n_test:]])\n",
    "\n",
    "        # Add remaining rows to train set\n",
    "        train_df = pd.concat([train_df, group.iloc[:-n_test]])\n",
    "\n",
    "    # Step 5: Reset index for both train and test dataframes\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    # TODO: normalize values\n",
    "\n",
    "    normalize_cols = [c for c in train_df.columns if c not in ['brand', 'date'] and not c.startswith(\"domicile_country_name\")]\n",
    "    for col in normalize_cols:\n",
    "        m = train_df[col].mean()\n",
    "        s = train_df[col].std()\n",
    "        train_df[col] = (train_df[col] - m)/s\n",
    "        test_df[col] = (test_df[col] - m)/s\n",
    "\n",
    "\n",
    "    # TODO: impute missing values\n",
    "  \n",
    "    # TODO: impute missing values\n",
    "    train_df = train_df.groupby('brand').apply(lambda group: group.fillna(method='ffill'))\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "    train_df = train_df.groupby('brand').apply(lambda group: group.fillna(method='bfill'))\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    test_df = test_df.groupby('brand').apply(lambda group: group.fillna(method='ffill'))\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "    test_df = test_df.groupby('brand').apply(lambda group: group.fillna(method='bfill'))\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "\n",
    "    return train_df, test_df\n",
    "\n",
    "\n",
    "def prepare_data_lstm(df, sequence_length=6, prediction_dist=8, missing_values_cutoff=0.7, test_fraction=0.2):\n",
    "\n",
    "    df = df.drop(columns=[\"period\", \"calculation_type\", \"compset\", \"compset_group\", \"legal_entity_name\", \"ultimate_parent_legal_entity_name\", \"primary_exchange_name\"])\n",
    "    df[\"period_end_date\"] = pd.to_datetime(df[\"period_end_date\"])\n",
    "\n",
    "    df = df.rename(columns={'business_entity_doing_business_as_name': 'brand', 'period_end_date': 'date'})\n",
    "\n",
    "    df = df[df['brand'] != \"All Brands\"]\n",
    "    df = df[df['brand'] != \"Boca\"]\n",
    "    df = df[df['brand'] != \"Bulgari Beauty\"]\n",
    "\n",
    "    df = df.groupby(['brand', 'date']).first().reset_index()\n",
    "\n",
    "    def correct_country_name(name):\n",
    "        country_map = {\n",
    "            \"Hong Kong\": \"China\",\n",
    "            \"China;Hong Kong\": \"China\",\n",
    "            \";France\": \"France\",\n",
    "            \";\": None,\n",
    "            \"Belgium;\": \"Belgium\"\n",
    "        }\n",
    "        if name in country_map.keys():\n",
    "            return country_map[name]\n",
    "        return name\n",
    "\n",
    "    df['domicile_country_name'] = df['domicile_country_name'].apply(correct_country_name)\n",
    "\n",
    "\n",
    "    categorical_features = [\"domicile_country_name\"]\n",
    "    for feature in categorical_features:\n",
    "        df = pd.get_dummies(df, columns=[feature], prefix=feature, dummy_na=True, dtype=int)\n",
    "\n",
    "    # TODO: remove bad data\n",
    "    na_frac = df[['brand', 'followers', 'pictures',\n",
    "        'videos', 'comments', 'likes']].groupby('brand').apply(lambda x: x.iloc[:,1:].isna().sum()/len(x))\n",
    "\n",
    "    bad_brands = list(na_frac[na_frac.max(axis=1) > missing_values_cutoff].index)\n",
    "\n",
    "    df = df[~df['brand'].isin(bad_brands)]\n",
    "\n",
    "\n",
    "    # TODO: add additional features\n",
    "    df['engagement'] = df['comments'] + df['likes']\n",
    "    df['engagement_rate'] = df['engagement']/df['followers']\n",
    "    df['engagement_rate_per_post'] =  df['engagement_rate']/(df['videos'] + df['pictures'])\n",
    "\n",
    "\n",
    "    # df = video_to_picture_ratio(df, \"videos\", \"pictures\")\n",
    "    df = calculate_rolling_average_per_brand(df, \"brand\", window_size=4)\n",
    "    df = calculate_exponential_moving_average_per_brand(df, \"brand\", span=4)\n",
    "    # df = calculate_brand_wise_growth_rates(df, [\"comments\", \"likes\", \"followers\", \"pictures\", \"videos\"], \"brand\")\n",
    "    for c in [\"comments\", \"likes\", \"followers\", \"pictures\", \"videos\"]:\n",
    "        df = calculate_brand_rolling_statistics(df, c, \"brand\", window_size=4)\n",
    "        df = create_brand_lag_features(df, c, \"brand\", 4)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # Step 1: Sort the dataframe by time\n",
    "    df_sorted = df.sort_values(by='date')\n",
    "\n",
    "    # Step 2: Group the dataframe by 'Brand'\n",
    "    grouped = df_sorted.groupby('brand')\n",
    "\n",
    "    # Step 3: Define an empty dataframe for train and test sets\n",
    "    train_df = pd.DataFrame(columns=df.columns)  # Columns same as original dataframe\n",
    "    test_df = pd.DataFrame(columns=df.columns)   # Columns same as original dataframe\n",
    "\n",
    "    # Step 4: Iterate over each group and split into train and test sets\n",
    "    for _, group in grouped:\n",
    "        n_rows = len(group)\n",
    "        n_test = int(test_fraction * n_rows)  # 20% of rows for test set\n",
    "\n",
    "        # Add last 20% of rows to test set\n",
    "        test_df = pd.concat([test_df, group.iloc[-n_test:]])\n",
    "\n",
    "        # Add remaining rows to train set\n",
    "        train_df = pd.concat([train_df, group.iloc[:-n_test]])\n",
    "\n",
    "    # Step 5: Reset index for both train and test dataframes\n",
    "    train_df.reset_index(drop=True, inplace=True)\n",
    "    test_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "    # TODO: normalize values\n",
    "\n",
    "    normalize_cols = [c for c in train_df.columns if c not in ['brand', 'date'] and not c.startswith(\"domicile_country_name\")]\n",
    "    for col in normalize_cols:\n",
    "        m = train_df[col].mean()\n",
    "        s = train_df[col].std()\n",
    "        train_df[col] = (train_df[col] - m)/s\n",
    "        test_df[col] = (test_df[col] - m)/s\n",
    "\n",
    "\n",
    "    # TODO: impute missing values\n",
    "  \n",
    "    # TODO: impute missing values\n",
    "    train_df = train_df.groupby('brand').apply(lambda group: group.fillna(method='ffill'))\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "    train_df = train_df.groupby('brand').apply(lambda group: group.fillna(method='bfill'))\n",
    "    train_df = train_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    test_df = test_df.groupby('brand').apply(lambda group: group.fillna(method='ffill'))\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "    test_df = test_df.groupby('brand').apply(lambda group: group.fillna(method='bfill'))\n",
    "    test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "\n",
    "    # TODO: create sequences\n",
    "\n",
    "    # TODO: split df into brands\n",
    "    # sequence per brand\n",
    "    # label\n",
    "\n",
    "    label_col = \"engagement_rate_per_post\"\n",
    "\n",
    "    train_sequences = []\n",
    "    train_labels = []\n",
    "    test_sequences = []\n",
    "    test_labels = []\n",
    "\n",
    "\n",
    "    for bi, brand in enumerate(df['brand'].unique()):\n",
    "\n",
    "        brand_train_df = train_df[train_df['brand'] == brand]\n",
    "\n",
    "        cols = [c for c in train_df.columns if (c != \"brand\" and c != \"date\")]\n",
    "        brand_train_df = brand_train_df[cols]\n",
    "\n",
    "        for i in range(len(brand_train_df) - (sequence_length + prediction_dist)):\n",
    "            sequence = brand_train_df.iloc[i:i + sequence_length].values\n",
    "            train_sequences.append(sequence)\n",
    "\n",
    "            label_seq = brand_train_df.iloc[i+sequence_length:i+sequence_length+prediction_dist]\n",
    "\n",
    "            virality = label_seq['engagement'].max() - label_seq['engagement'].mean()\n",
    "            growth = ((label_seq.iloc[-1] - label_seq.iloc[0]['followers'])/label_seq.iloc[0]['followers'])**3\n",
    "\n",
    "            label = virality + growth\n",
    "\n",
    "            train_labels.append(label)\n",
    "        \n",
    "        brand_test_df = test_df[test_df['brand'] == brand]\n",
    "\n",
    "        brand_test_df = brand_test_df[cols]\n",
    "\n",
    "\n",
    "        for i in range(len(brand_test_df) - (sequence_length + prediction_dist)):\n",
    "            sequence = brand_test_df.iloc[i:i + sequence_length].values\n",
    "            if (np.isnan(sequence).any()):\n",
    "                continue\n",
    "                \n",
    "            test_sequences.append(sequence)\n",
    "\n",
    "            label_seq = brand_test_df.iloc[i+sequence_length:i+sequence_length+prediction_dist]\n",
    "\n",
    "            virality = label_seq['engagement'].max() - label_seq['engagement'].mean()\n",
    "            growth = ((label_seq.iloc[-1] - label_seq.iloc[0]['followers'])/label_seq.iloc[0]['followers'])**3\n",
    "\n",
    "            label = virality + growth\n",
    "\n",
    "            test_labels.append(label)\n",
    "\n",
    "    return train_sequences, train_labels, test_sequences, test_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = preprocess_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_16277/1692026652.py:255: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  'videos', 'comments', 'likes']].groupby('brand').apply(lambda x: x.iloc[:,1:].isna().sum()/len(x))\n",
      "/tmp/ipykernel_16277/1692026652.py:299: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_df = pd.concat([test_df, group.iloc[-n_test:]])\n",
      "/tmp/ipykernel_16277/1692026652.py:302: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  train_df = pd.concat([train_df, group.iloc[:-n_test]])\n",
      "/tmp/ipykernel_16277/1692026652.py:322: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_df = train_df.groupby('brand').apply(lambda group: group.fillna(method='ffill'))\n",
      "/tmp/ipykernel_16277/1692026652.py:322: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_df = train_df.groupby('brand').apply(lambda group: group.fillna(method='ffill'))\n",
      "/tmp/ipykernel_16277/1692026652.py:322: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_df = train_df.groupby('brand').apply(lambda group: group.fillna(method='ffill'))\n",
      "/tmp/ipykernel_16277/1692026652.py:325: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_df = train_df.groupby('brand').apply(lambda group: group.fillna(method='bfill'))\n",
      "/tmp/ipykernel_16277/1692026652.py:325: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_df = train_df.groupby('brand').apply(lambda group: group.fillna(method='bfill'))\n",
      "/tmp/ipykernel_16277/1692026652.py:329: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df = test_df.groupby('brand').apply(lambda group: group.fillna(method='ffill'))\n",
      "/tmp/ipykernel_16277/1692026652.py:329: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_df = test_df.groupby('brand').apply(lambda group: group.fillna(method='ffill'))\n",
      "/tmp/ipykernel_16277/1692026652.py:329: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_df = test_df.groupby('brand').apply(lambda group: group.fillna(method='ffill'))\n",
      "/tmp/ipykernel_16277/1692026652.py:332: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df = test_df.groupby('brand').apply(lambda group: group.fillna(method='bfill'))\n",
      "/tmp/ipykernel_16277/1692026652.py:332: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_df = test_df.groupby('brand').apply(lambda group: group.fillna(method='bfill'))\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m X_train, y_train, X_test, y_test \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_data_lstm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[28], line 364\u001b[0m, in \u001b[0;36mprepare_data_lstm\u001b[0;34m(df, sequence_length, prediction_dist, missing_values_cutoff, test_fraction)\u001b[0m\n\u001b[1;32m    361\u001b[0m label_seq \u001b[38;5;241m=\u001b[39m brand_train_df\u001b[38;5;241m.\u001b[39miloc[i\u001b[38;5;241m+\u001b[39msequence_length:i\u001b[38;5;241m+\u001b[39msequence_length\u001b[38;5;241m+\u001b[39mprediction_dist]\n\u001b[1;32m    363\u001b[0m virality \u001b[38;5;241m=\u001b[39m label_seq[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mengagement\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmax() \u001b[38;5;241m-\u001b[39m label_seq[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mengagement\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m--> 364\u001b[0m growth \u001b[38;5;241m=\u001b[39m ((\u001b[43mlabel_seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabel_seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfollowers\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\u001b[38;5;241m/\u001b[39mlabel_seq\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfollowers\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m3\u001b[39m\n\u001b[1;32m    366\u001b[0m label \u001b[38;5;241m=\u001b[39m virality \u001b[38;5;241m+\u001b[39m growth\n\u001b[1;32m    368\u001b[0m train_labels\u001b[38;5;241m.\u001b[39mappend(label)\n",
      "File \u001b[0;32m~/anaconda3/envs/datathon24/lib/python3.11/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/datathon24/lib/python3.11/site-packages/pandas/core/arraylike.py:194\u001b[0m, in \u001b[0;36mOpsMixin.__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__sub__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__sub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 194\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/datathon24/lib/python3.11/site-packages/pandas/core/series.py:6135\u001b[0m, in \u001b[0;36mSeries._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   6133\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_arith_method\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, op):\n\u001b[1;32m   6134\u001b[0m     \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other)\n\u001b[0;32m-> 6135\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mIndexOpsMixin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/datathon24/lib/python3.11/site-packages/pandas/core/base.py:1384\u001b[0m, in \u001b[0;36mIndexOpsMixin._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   1381\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1382\u001b[0m     result \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39marithmetic_op(lvalues, rvalues, op)\n\u001b[0;32m-> 1384\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_construct_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mres_name\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/datathon24/lib/python3.11/site-packages/pandas/core/series.py:6231\u001b[0m, in \u001b[0;36mSeries._construct_result\u001b[0;34m(self, result, name)\u001b[0m\n\u001b[1;32m   6228\u001b[0m \u001b[38;5;66;03m# TODO: result should always be ArrayLike, but this fails for some\u001b[39;00m\n\u001b[1;32m   6229\u001b[0m \u001b[38;5;66;03m#  JSONArray tests\u001b[39;00m\n\u001b[1;32m   6230\u001b[0m dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 6231\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_constructor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   6232\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m   6234\u001b[0m \u001b[38;5;66;03m# Set the result's name after __finalize__ is called because __finalize__\u001b[39;00m\n\u001b[1;32m   6235\u001b[0m \u001b[38;5;66;03m#  would set it back to self.name\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/datathon24/lib/python3.11/site-packages/pandas/core/series.py:584\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    582\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m    583\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 584\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[43msanitize_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    586\u001b[0m     manager \u001b[38;5;241m=\u001b[39m _get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.data_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m, silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    587\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m manager \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/datathon24/lib/python3.11/site-packages/pandas/core/construction.py:555\u001b[0m, in \u001b[0;36msanitize_array\u001b[0;34m(data, index, dtype, copy, allow_2d)\u001b[0m\n\u001b[1;32m    552\u001b[0m     object_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# extract ndarray or ExtensionArray, ensure we have no NumpyExtensionArray\u001b[39;00m\n\u001b[0;32m--> 555\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mextract_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_numpy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextract_range\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, np\u001b[38;5;241m.\u001b[39mndarray) \u001b[38;5;129;01mand\u001b[39;00m data\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    558\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/datathon24/lib/python3.11/site-packages/pandas/core/construction.py:416\u001b[0m, in \u001b[0;36mextract_array\u001b[0;34m(obj, extract_numpy, extract_range)\u001b[0m\n\u001b[1;32m    409\u001b[0m \u001b[38;5;129m@overload\u001b[39m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_array\u001b[39m(\n\u001b[1;32m    411\u001b[0m     obj: T, extract_numpy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, extract_range: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    412\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T \u001b[38;5;241m|\u001b[39m ArrayLike:\n\u001b[1;32m    413\u001b[0m     \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m--> 416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mextract_array\u001b[39m(\n\u001b[1;32m    417\u001b[0m     obj: T, extract_numpy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, extract_range: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    418\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T \u001b[38;5;241m|\u001b[39m ArrayLike:\n\u001b[1;32m    419\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;124;03m    Extract the ndarray or ExtensionArray from a Series or Index.\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;124;03m    array([1, 2, 3])\u001b[39;00m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     typ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_typ\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train, y_train, X_test, y_test = prepare_data_lstm(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_5891/2877952213.py:143: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  'videos', 'comments', 'likes']].groupby('brand').apply(lambda x: x.iloc[:,1:].isna().sum()/len(x))\n",
      "/tmp/ipykernel_5891/2877952213.py:181: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  test_df = pd.concat([test_df, group.iloc[-n_test:]])\n",
      "/tmp/ipykernel_5891/2877952213.py:184: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  train_df = pd.concat([train_df, group.iloc[:-n_test]])\n",
      "/tmp/ipykernel_5891/2877952213.py:204: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_df = train_df.groupby('brand').apply(lambda group: group.fillna(method='ffill'))\n",
      "/tmp/ipykernel_5891/2877952213.py:204: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  train_df = train_df.groupby('brand').apply(lambda group: group.fillna(method='ffill'))\n",
      "/tmp/ipykernel_5891/2877952213.py:204: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_df = train_df.groupby('brand').apply(lambda group: group.fillna(method='ffill'))\n",
      "/tmp/ipykernel_5891/2877952213.py:207: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  train_df = train_df.groupby('brand').apply(lambda group: group.fillna(method='bfill'))\n",
      "/tmp/ipykernel_5891/2877952213.py:207: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  train_df = train_df.groupby('brand').apply(lambda group: group.fillna(method='bfill'))\n",
      "/tmp/ipykernel_5891/2877952213.py:211: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df = test_df.groupby('brand').apply(lambda group: group.fillna(method='ffill'))\n",
      "/tmp/ipykernel_5891/2877952213.py:211: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  test_df = test_df.groupby('brand').apply(lambda group: group.fillna(method='ffill'))\n",
      "/tmp/ipykernel_5891/2877952213.py:211: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_df = test_df.groupby('brand').apply(lambda group: group.fillna(method='ffill'))\n",
      "/tmp/ipykernel_5891/2877952213.py:214: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  test_df = test_df.groupby('brand').apply(lambda group: group.fillna(method='bfill'))\n",
      "/tmp/ipykernel_5891/2877952213.py:214: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  test_df = test_df.groupby('brand').apply(lambda group: group.fillna(method='bfill'))\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(DATA_PATH, sep=\";\")\n",
    "train, test = preprocess_data(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datathon24",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
